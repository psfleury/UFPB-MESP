# -*- coding: utf-8 -*-
"""Avaliacao_de_Modelos_vf3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hwPc0_3PqPym1JwYE8bjGkO8z2Nqct7U
"""

from google.colab import drive

# Montar o Google Drive
drive.mount('/content/drive')

# Script de consulta da tabela consolidada com armazenamento em dataframe do pandas

import sqlite3
import pandas as pd

# Conectar ao banco de dados SQLite
conn = sqlite3.connect('/content/drive/MyDrive/MESP_Previdencia/mesp_previdencia.db')

# Criar um objeto cursor para executar comandos SQL
cursor = conn.cursor()

# Exemplo de consulta SELECT
consulta = """SELECT * from tabela_pretreinamento;
"""

dados_final = pd.read_sql_query(consulta, conn)

# Fechar a conexão
conn.close()

dados_final.sample(5)

# Importar as bibliotecas necessárias à análise. Em suma: pandas, numpy, matplotlib, nltk, sklearn (diversos) e random
import numpy as np
import pandas as pd
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder
from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm, tree, neighbors
from sklearn.ensemble import RandomForestClassifier
from sklearn.compose import ColumnTransformer
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from random import seed
import matplotlib.pyplot as plt
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.ensemble import BaggingClassifier

# Transformar variáveis categóricas em tipo "category"
dados_final['nr_proc'] = dados_final['nr_proc'].astype("category")
dados_final['jurisdicionado'] = dados_final['jurisdicionado'].astype("category")
dados_final['ano'] = dados_final['ano'].astype("category")
dados_final['orgao'] = dados_final['orgao'].astype("category")
dados_final['carreira'] = dados_final['carreira'].astype("category")
dados_final['cargo'] = dados_final['cargo'].astype("category")
dados_final['flag_aposesp'] = dados_final['flag_aposesp'].astype("category")
dados_final['meio_publi'] = dados_final['meio_publi'].astype("category")
dados_final['regra_apos'] = dados_final['regra_apos'].astype("category")
dados_final['qtd_vinculos'] = dados_final['qtd_vinculos'].astype("category")
dados_final['qtd_vinculos_pub'] = dados_final['qtd_vinculos_pub'].astype("category")
dados_final['qtd_vinculos_priv'] = dados_final['qtd_vinculos_priv'].astype("category")
dados_final['qtd_vinculos_mesmo_cargo'] = dados_final['qtd_vinculos_mesmo_cargo'].astype("category")
dados_final['qtd_vinculos_mesma_carreira'] = dados_final['qtd_vinculos_mesma_carreira'].astype("category")
dados_final['p_vencimento'] = dados_final['p_vencimento'].astype("category")
dados_final['p_ats'] = dados_final['p_ats'].astype("category")
dados_final['p_proventos'] = dados_final['p_proventos'].astype("category")
dados_final['p_gratificacao'] = dados_final['p_gratificacao'].astype("category")
dados_final['p_vantpessoal'] = dados_final['p_vantpessoal'].astype("category")
dados_final['p_insalubridade'] = dados_final['p_insalubridade'].astype("category")
dados_final['p_adcqualificacao'] = dados_final['p_adcqualificacao'].astype("category")
dados_final['p_representacao'] = dados_final['p_representacao'].astype("category")
dados_final['p_antecipacao'] = dados_final['p_antecipacao'].astype("category")
dados_final['p_indenizatorio'] = dados_final['p_indenizatorio'].astype("category")
dados_final['ind_sm'] = dados_final['ind_sm'].astype("category")

global cons_transformed_df, cons_tabela_5, cons_tabela_10, cons_tabela_11, cons_tabela_12
cons_transformed_df = pd.DataFrame()
cons_tabela_5 = pd.DataFrame()
cons_tabela_10 = pd.DataFrame()
cons_tabela_11 = pd.DataFrame()
cons_tabela_12 = pd.DataFrame()

for rnd_ind in range(0, 150):
    # Separar dataset em bases de treino (train) e teste (valid)
    # mantendo proporção dos labels (estratify) e com uma seed aleatoriamente escolhida
    features = dados_final[['nr_proc', 'jurisdicionado', 'ano', 'orgao', 'carreira', 'cargo', 'flag_aposesp', 'meio_publi', 'regra_apos', 'qtd_vinculos', 'qtd_vinculos_pub', 'qtd_vinculos_priv', 'qtd_vinculos_mesmo_cargo', 'qtd_vinculos_mesma_carreira', 'p_vencimento', 'p_ats', 'p_proventos', 'p_gratificacao', 'p_vantpessoal', 'p_insalubridade', 'p_adcqualificacao', 'p_representacao', 'p_antecipacao', 'p_indenizatorio', 'ind_sm']]
    label = dados_final['rotulo']
    x_train, x_valid, y_train, y_valid = model_selection.train_test_split(features, label, random_state=rnd_ind, stratify=label)

    # Parametrizando o pré-processamento das features

    categorical_features = ['nr_proc', 'jurisdicionado', 'ano', 'orgao', 'carreira', 'cargo', 'flag_aposesp', 'meio_publi', 'regra_apos', 'qtd_vinculos', 'qtd_vinculos_pub', 'qtd_vinculos_priv', 'qtd_vinculos_mesmo_cargo', 'qtd_vinculos_mesma_carreira', 'p_vencimento', 'p_ats', 'p_proventos', 'p_gratificacao', 'p_vantpessoal', 'p_insalubridade', 'p_adcqualificacao', 'p_representacao', 'p_antecipacao', 'p_indenizatorio', 'ind_sm']
    categorical_transformer = OneHotEncoder(handle_unknown="ignore")

    preprocessor = ColumnTransformer(
        transformers=[
            ("categorical columns", categorical_transformer, categorical_features)
        ]
    )

    global global_df
    global_df = pd.DataFrame({'nr_proc': x_valid['nr_proc'], 'y_valid': y_valid})

    # Criar um DataFrame global vazio
    confusion_matrix_df = pd.DataFrame(columns=['Model', 'TN', 'FP', 'FN', 'TP'])

    # Função para adicionar uma matriz de confusão ao DataFrame global
    def add_confusion_matrix(model_name, confusion_matrix):
        global confusion_matrix_df

        # Extrair os valores da matriz de confusão
        TN, FP, FN, TP = confusion_matrix.ravel()

        # Criar um DataFrame temporário para a matriz de confusão atual
        df = pd.DataFrame({
            'Model': [model_name],
            'TN': [TN],
            'FP': [FP],
            'FN': [FN],
            'TP': [TP]
        })

        confusion_matrix_df = pd.concat([confusion_matrix_df, df], ignore_index=True)

    # Cria função que parametriza o treinamento de um modelo definido como input, faz suas predições, cria a matriz de confusão e preenche um dataframe global de informações importantes para o modelo rodado

    def train_model(classifier, feature_vector_train, label, feature_vector_valid, label_valid, titulo_mapa, is_neural_net=False):
      clf = Pipeline(steps=[("preprocessor", preprocessor), ("classifier", classifier)])
      clf.fit(feature_vector_train, label)
      pred_test = clf.predict(feature_vector_valid)
      conf_mat = confusion_matrix(label_valid, pred_test)
      add_confusion_matrix(titulo_mapa, conf_mat)
      global_df[titulo_mapa] = pred_test


    import numpy as np
    import pandas as pd
    import seaborn as sns
    from sklearn.model_selection import train_test_split
    from sklearn.pipeline import Pipeline
    from sklearn.preprocessing import OneHotEncoder
    from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm, tree, neighbors
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.compose import ColumnTransformer
    from sklearn.metrics import confusion_matrix
    from sklearn.metrics import classification_report
    from random import seed
    import matplotlib.pyplot as plt
    from sklearn.neural_network import MLPClassifier
    from sklearn.ensemble import AdaBoostClassifier
    from sklearn.ensemble import GradientBoostingClassifier
    from sklearn.ensemble import ExtraTreesClassifier
    from sklearn.ensemble import BaggingClassifier

    ### Função das previsões do Comitê Classificador:
    classificador={
                   "Logistic Regression":linear_model.LogisticRegression(max_iter = 50000),
                   "Naive Bayes": naive_bayes.MultinomialNB(),
                   "KNN": neighbors.KNeighborsClassifier(n_neighbors=5, algorithm='auto'),
                   "SVM": svm.SVC(),
                   "Decision Tree": tree.DecisionTreeClassifier(random_state=rnd_ind),
                   "Random Forest":RandomForestClassifier(random_state=rnd_ind),
                   "Bagging":BaggingClassifier(random_state=rnd_ind),
                   "MLPClassifier": MLPClassifier(hidden_layer_sizes=(15,15,15,15), activation='relu', solver='adam', alpha=0.0001,
                                                 batch_size='auto', learning_rate='constant', learning_rate_init=0.001,
                                                 power_t=0.5, max_iter=200000, shuffle=True, random_state=rnd_ind, tol=0.0001,
                                                 verbose=False, warm_start=True, momentum=0.9, nesterovs_momentum=True, early_stopping=False,
                                                 validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=100, max_fun=15000),
                   "ADA": AdaBoostClassifier(n_estimators=100, algorithm="SAMME",),
                   "Gradient Boost": GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=rnd_ind),
                   "Extratree"     : ExtraTreesClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=rnd_ind)
                }
    for i , (clf_name, classifier) in enumerate (classificador.items()):
        train_model(classifier, x_train, y_train, x_valid, y_valid, clf_name)

    global_df_adjust = global_df.copy()
    colunas_nao_binarias = ['nr_proc', 'y_valid']
    colunas_binarias = [coluna for coluna in global_df_adjust.columns if coluna not in colunas_nao_binarias]
    global_df_adjust[colunas_binarias] = global_df_adjust[colunas_binarias].replace(0, -1)

    global_df_adjust['Soma_Modelos']=global_df_adjust[colunas_binarias].sum(axis=1)
    colunas_nao_binarias = ['nr_proc', 'y_valid', 'Soma_Modelos']
    colunas_binarias = [coluna for coluna in global_df_adjust.columns if coluna not in colunas_nao_binarias]
    global_df_adjust[colunas_binarias] = global_df_adjust[colunas_binarias].replace(-1, 0)
    global_df_adjust['Modulo_Soma'] = global_df_adjust['Soma_Modelos'].abs()
    global_df_adjust['Class_Coletiva'] = global_df_adjust.Soma_Modelos.apply(lambda x: 1 if x >=0 else 0)


    import pandas as pd
    from sklearn.metrics import classification_report, confusion_matrix
    import numpy as np

    # Crie um DataFrame vazio para armazenar os resultados
    df_consolidado = pd.DataFrame()

    # Mapeamento de nomes dos subconjuntos de dados
    comite_names = {
        11: "Subconjunto Consenso 11",
        9: "Subconjunto Consenso 10",
        7: "Subconjunto Consenso 9",
        5: "Subconjunto Consenso 8",
        3: "Subconjunto Consenso 7",
        1: "Subconjunto Consenso 6"
    }

    # Nome das colunas dos modelos preditivos
    model_columns = [
        'Logistic Regression', 'Naive Bayes', 'KNN', 'SVM',
        'Decision Tree', 'Random Forest', 'Bagging',
        'MLPClassifier', 'ADA', 'Gradient Boost', 'Extratree'
    ]

    # Iterar sobre os possíveis valores de módulo soma (modulo_soma = 11 é consenso unânime, =9 é consenso de 10, =7 consenso de 9, =5 consenso de 8, =3 consenso de 7, =1 consenso de 6)
    for i in range(11, 0, -2):
        # Selecionar registros que compõem o subconjunto
        transfor_n = global_df_adjust.loc[global_df_adjust['Modulo_Soma'] == i].copy()

        # Aplicar a transformação nos valores de Soma_Modelos
        transfor_n_report = transfor_n['Soma_Modelos'].apply(lambda x: 1 if x >= 0 else 0)

        # Gerar o relatório de classificação para a escolha coletiva do subconjunto de dados
        dicio = classification_report(transfor_n['y_valid'].values, transfor_n_report.values, digits=6, output_dict=True)
        dicio.pop("accuracy")
        dicio.pop("macro avg")
        dicio.pop("weighted avg")

        # Gerar a matriz de confusão para a escolha coletiva
        conf_mat_cs = confusion_matrix(transfor_n['y_valid'], transfor_n_report)
        comite_name = comite_names.get(i)
        add_confusion_matrix(comite_name, conf_mat_cs)

        # Transformar o dicionário resultante em DataFrame e adicionar uma coluna para a iteração
        df_temp = pd.DataFrame(dicio).T
        df_temp["Modelo"] = comite_name
        df_temp["Classe"] = df_temp.index

        # Concatenar o DataFrame temporário ao DataFrame consolidado
        df_consolidado = pd.concat([df_consolidado, df_temp])

        # Calcular a matriz de confusão para cada modelo individual no subconjunto de dados
        for model in model_columns:
            y_pred = transfor_n[model]
            y_true = transfor_n['y_valid']
            conf_matrix = confusion_matrix(y_true, y_pred)
            add_confusion_matrix(f"{comite_name} - {model}", conf_matrix)

            # Gerar o relatório de classificação para cada modelo individual
            dicio_model = classification_report(y_true, y_pred, digits=6, output_dict=True)
            dicio_model.pop("accuracy")
            dicio_model.pop("macro avg")
            dicio_model.pop("weighted avg")

            # Transformar o dicionário resultante em DataFrame e adicionar uma coluna para a iteração
            df_temp_model = pd.DataFrame(dicio_model).T
            df_temp_model["Modelo"] = f"{comite_name} - {model}"
            df_temp_model["Classe"] = df_temp_model.index

            # Concatenar o DataFrame temporário ao DataFrame consolidado
            df_consolidado = pd.concat([df_consolidado, df_temp_model])

    import pandas as pd

    rl = classification_report(y_valid, global_df['Logistic Regression'], digits=6, output_dict=True)
    nb = classification_report(y_valid, global_df['Naive Bayes'], digits=6, output_dict=True)
    knn = classification_report(y_valid, global_df['KNN'], digits=6, output_dict=True)
    svm = classification_report(y_valid, global_df['SVM'], digits=6, output_dict=True)
    ad = classification_report(y_valid, global_df['Decision Tree'], digits=6, output_dict=True)
    fr = classification_report(y_valid, global_df['Random Forest'], digits=6, output_dict=True)
    bag = classification_report(y_valid, global_df['Bagging'], digits=6, output_dict=True)
    mlpc = classification_report(y_valid, global_df['MLPClassifier'], digits=6, output_dict=True)
    ada = classification_report(y_valid, global_df['ADA'], digits=6, output_dict=True)
    gboo = classification_report(y_valid, global_df['Gradient Boost'], digits=6, output_dict=True)
    xt = classification_report(y_valid, global_df['Extratree'], digits=6, output_dict=True)

    # Consolidar os dicionários em um único dicionário de dicionários
    reports_dict = {'Logistic Regression': rl,
                    'Naive Bayes': nb,
                    'KNN': knn,
                    'SVM': svm,
                    'Decision Tree': ad,
                    'Random Forest': fr,
                    'Bagging': bag,
                    'MLPClassifier': mlpc,
                    'ADA': ada,
                    'Gradient Boost': gboo,
                    'Extratree': xt}

    # Lista para armazenar os dados consolidados
    consolidated_data = []

    # Iterar sobre os dicionários de classification_report de cada modelo
    for model_name, report_dict in reports_dict.items():
        # Iterar sobre as classes de cada modelo
        for class_name, metrics_dict in report_dict.items():
            # Verificar se metrics_dict é um dicionário válido
            if isinstance(metrics_dict, dict):
                # Adicionar um novo dicionário com os indicadores, incluindo o nome do modelo e o nome da classe
                consolidated_data.append({'Modelo': model_name, 'Classe': class_name, **metrics_dict})


    # Criar DataFrame a partir dos dados consolidados
    consolidated_df = pd.DataFrame(consolidated_data)

    consolidated_df = consolidated_df.loc[consolidated_df['Classe'].isin(['0', '1'])]

    df_final = pd.concat([consolidated_df, df_consolidado], ignore_index=True)

    import pandas as pd

    # Transformar o DataFrame usando pivot_table
    transformed_df = df_final.pivot_table(index='Modelo', columns='Classe', values=['precision', 'recall', 'f1-score', 'support'])

    # Renomear as colunas para refletir as classes
    transformed_df.columns = [f'{indicator}_{class_}' for indicator, class_ in transformed_df.columns]

    # Resetar o índice para garantir que 'Modelo' seja uma coluna
    transformed_df = transformed_df.reset_index()

    # Mesclar os DataFrames usando os campos de cruzamento diferentes
    transformed_df = pd.merge(transformed_df, confusion_matrix_df, left_on='Modelo', right_on='Model')

    ordem_colunas = ['Modelo','TN', 'FP', 'FN', 'TP', 'f1-score_0', 'f1-score_1', 'precision_0', 'precision_1', 'recall_0', 'recall_1', 'support_0', 'support_1']

    transformed_df = transformed_df[ordem_colunas]

    test_base_size = y_valid.shape[0]
    transformed_df["Beneficio"] = transformed_df["TN"]
    transformed_df["Custo"] = transformed_df["FN"]
    transformed_df["Beneficio/Custo"] = transformed_df["Beneficio"] / transformed_df["Custo"]
    coluna_para_ordenar = 'Beneficio/Custo'
    transformed_df = transformed_df.sort_values(by=coluna_para_ordenar, ascending=False)

    transformed_df['Accuracy'] = (transformed_df['TN'] + transformed_df['TP']) / (transformed_df['support_0'] + transformed_df['support_1'])
    transformed_df = transformed_df.drop_duplicates()

    transf_df_iteracao = transformed_df.copy()

    # Adiciona uma coluna 'Iteracao' para indicar o número da iteração
    transf_df_iteracao['Iteracao'] = rnd_ind

    # Concatenar a tabela_5 ao cons_tabela_5
    cons_transformed_df = pd.concat([cons_transformed_df, transf_df_iteracao], ignore_index=True)

    # Cria uma cópia do DataFrame transformed_df
    tabela_5 = transformed_df.copy()

    # Remove as colunas que contêm "f1-score", "precision", "recall", "Beneficio", "Custo"
    colunas_para_remover = [coluna for coluna in tabela_5.columns if any(substring in coluna for substring in ["f1-score", "precision", "recall", "Beneficio", "Custo"])]
    tabela_5 = tabela_5.drop(columns=colunas_para_remover)

    # Filtra os registros para manter apenas os que contêm "Subconjunto Consenso" na coluna "Modelo"
    tabela_5 = tabela_5[tabela_5['Modelo'].str.contains('Subconjunto Consenso')]
    tabela_5 = tabela_5[~tabela_5['Modelo'].str.contains("-")]
    tabela_5['Total de Processos'] = tabela_5['support_0'] + tabela_5['support_1']
    tabela_5['Total de Processos']=tabela_5['Total de Processos'].astype(int)

    colunas_para_remover = [coluna for coluna in tabela_5.columns if any(substring in coluna for substring in ["support", "Accuracy"])]
    tabela_5 = tabela_5.drop(columns=colunas_para_remover)

    # Cria uma máscara para selecionar as linhas com 6, 7 ou 8 na coluna 'Modelo'
    mask = tabela_5['Modelo'].str.contains('6|7|8')

    # Calcula a soma das linhas selecionadas
    linha_agregadora = tabela_5[mask].sum()

    # Cria um novo DataFrame com a linha agregadora
    linha_agregadora_df = pd.DataFrame(linha_agregadora).transpose()
    linha_agregadora_df['Modelo'] = 'Subconjunto 8 ou menos'

    # Remove as linhas originais com 6, 7 ou 8 na coluna 'Modelo'
    tabela_5 = tabela_5[~mask]

    # Adiciona a linha agregadora ao DataFrame original
    tabela_5 = pd.concat([tabela_5, linha_agregadora_df], ignore_index=True)

    # Reordena as colunas para que 'Modelo' seja a primeira
    tabela_5 = tabela_5[['Modelo', 'TN', 'FP', 'FN', 'TP', 'Total de Processos']]

    # Calcula a soma da coluna "Total de Processos"
    total_processos = tabela_5['Total de Processos'].sum()

    # Cria a coluna "% da base de teste" e arredonda para 1 casa decimal
    tabela_5['% da base de teste'] = (tabela_5['Total de Processos'].fillna(0) / total_processos * 100).round(2)

    # Cria a coluna "Beneficio / Custo" e arredonda para 2 casas decimais
    tabela_5['Beneficio / Custo'] = (tabela_5['TN'].astype(float) / tabela_5['FN'].astype(float)).round(3)

    # Adiciona uma coluna 'Iteracao' para indicar o número da iteração
    tabela_5['Iteracao'] = rnd_ind

    # Concatenar a tabela_5 ao cons_tabela_5
    cons_tabela_5 = pd.concat([cons_tabela_5, tabela_5], ignore_index=True)


    # Cria uma cópia do DataFrame transformed_df
    tabela_6 = transformed_df.copy()

    # Filtra os registros para manter apenas os que contêm "-" na coluna "Modelo"
    tabela_6 = tabela_6[tabela_6['Modelo'].str.contains("11 -")]
    tabela_6['Modelo'] = tabela_6['Modelo'].str.split(' - ', expand=True)[1]


    # Cria coluna total de processos
    tabela_6['Total de Processos'] = tabela_6['support_0'] + tabela_6['support_1']
    tabela_6['Total de Processos']= tabela_6['Total de Processos'].astype(int)

    # Remove as colunas que contêm "f1-score", "precision", "recall", "Beneficio", "Custo"
    colunas_para_remover = [coluna for coluna in tabela_6.columns if any(substring in coluna for substring in ["f1-score", "precision", "recall", "Beneficio", "Custo", "Accuracy", "support"])]
    tabela_6 = tabela_6.drop(columns=colunas_para_remover)

    # Calcula a soma da coluna "Total de Processos"
    total_processos = tabela_6['Total de Processos'].sum()

    # Cria a coluna "Beneficio / Custo" e arredonda para 2 casas decimais
    tabela_6['Beneficio / Custo'] = (tabela_6['TN'].astype(float) / tabela_6['FN'].astype(float)).round(3)

    # Cria uma cópia do DataFrame transformed_df
    tabela_7 = transformed_df.copy()

    # Filtra os registros para manter apenas os que contêm "-" na coluna "Modelo"
    tabela_7 = tabela_7[tabela_7['Modelo'].str.contains("10 -")]
    tabela_7['Modelo'] = tabela_7['Modelo'].str.split(' - ', expand=True)[1]

    # Cria coluna total de processos
    tabela_7['Total de Processos'] = tabela_7['support_0'] + tabela_7['support_1']
    tabela_7['Total de Processos']= tabela_7['Total de Processos'].astype(int)

    # Remove as colunas que contêm "f1-score", "precision", "recall", "Beneficio", "Custo"
    colunas_para_remover = [coluna for coluna in tabela_7.columns if any(substring in coluna for substring in ["f1-score", "precision", "recall", "Beneficio", "Custo", "Accuracy", "support"])]
    tabela_7 = tabela_7.drop(columns=colunas_para_remover)

    # Calcula a soma da coluna "Total de Processos"
    total_processos = tabela_7['Total de Processos'].sum()

    # Cria a coluna "Beneficio / Custo" e arredonda para 2 casas decimais
    tabela_7['Beneficio / Custo'] = (tabela_7['TN'].astype(float) / tabela_7['FN'].astype(float)).round(3)

    # Cria uma cópia do DataFrame transformed_df
    tabela_8 = transformed_df.copy()

    # Filtra os registros para manter apenas os que contêm "-" na coluna "Modelo"
    tabela_8 = tabela_8[tabela_8['Modelo'].str.contains("9 -")]

    tabela_8['Modelo'] = tabela_8['Modelo'].str.split(' - ', expand=True)[1]

    # Cria coluna total de processos
    tabela_8['Total de Processos'] = tabela_8['support_0'] + tabela_8['support_1']
    tabela_8['Total de Processos']= tabela_8['Total de Processos'].astype(int)

    # Remove as colunas que contêm "f1-score", "precision", "recall", "Beneficio", "Custo"
    colunas_para_remover = [coluna for coluna in tabela_8.columns if any(substring in coluna for substring in ["f1-score", "precision", "recall", "Beneficio", "Custo", "Accuracy", "support"])]
    tabela_8 = tabela_8.drop(columns=colunas_para_remover)

    # Calcula a soma da coluna "Total de Processos"
    total_processos = tabela_8['Total de Processos'].sum()

    # Cria a coluna "Beneficio / Custo" e arredonda para 2 casas decimais
    tabela_8['Beneficio / Custo'] = (tabela_8['TN'].astype(float) / tabela_8['FN'].astype(float)).round(3)

    # Cria uma cópia do DataFrame transformed_df
    tabela_9 = transformed_df.copy()

    # Filtra os registros para manter apenas os que contêm "-" na coluna "Modelo"
    tabela_9 = tabela_9[tabela_9['Modelo'].str.contains(r'8 -|7 -|6 -')]

    tabela_9['Modelo'] = tabela_9['Modelo'].str.split(' - ', expand=True)[1]

    # Cria coluna total de processos
    tabela_9['Total de Processos'] = tabela_9['support_0'] + tabela_9['support_1']
    tabela_9['Total de Processos']= tabela_9['Total de Processos'].astype(int)

    # Remove as colunas que contêm "f1-score", "precision", "recall", "Beneficio", "Custo"
    colunas_para_remover = [coluna for coluna in tabela_9.columns if any(substring in coluna for substring in ["f1-score", "precision", "recall", "Beneficio", "Custo", "Accuracy", "support"])]
    tabela_9 = tabela_9.drop(columns=colunas_para_remover)

    tabela_9 = tabela_9.groupby('Modelo', as_index=False).sum()

    # Cria a coluna "Beneficio / Custo" e arredonda para 2 casas decimais
    tabela_9['Beneficio / Custo'] = (tabela_9['TN'].astype(float) / tabela_9['FN'].astype(float)).round(3)

    tabela_9 = tabela_9.sort_values(by='Beneficio / Custo', ascending=False)

    # Cria uma cópia da tabela_5
    tabela_10 = tabela_5.copy()

    tabela_10['teste'] = tabela_10.apply(
        lambda row: row['Beneficio / Custo'] if row['Modelo'] != 'Subconjunto 8 ou menos' else 1.34,
        axis=1
    )
    tabela_10['Beneficio / Custo']=tabela_10['teste']
    tabela_10 = tabela_10.drop(columns=['teste', 'TN', 'FP', 'FN', 'TP'])

    # Cria a coluna "Composição Acumulada (%)"
    tabela_10['Composição Acumulada (%)'] = tabela_10['% da base de teste'].cumsum()

    # Cria a coluna "Escolha" com base na condição
    tabela_10['Escolha'] = tabela_10.apply(
        lambda row: 'Coletiva' if row['Modelo'] != 'Subconjunto 8 ou menos' else 'Aleatória',
        axis=1
    )

    # Renomeia a coluna "Modelo" para "Grau de consenso"
    tabela_10 = tabela_10.rename(columns={'Modelo': 'Grau de consenso'})

    # Reordena as colunas
    tabela_10 = tabela_10[['Grau de consenso', 'Escolha', 'Total de Processos', '% da base de teste', 'Composição Acumulada (%)', 'Beneficio / Custo']]

    # Calcular o valor ponderado da coluna "Beneficio / Custo" em função da coluna "Total de Processos"
    valor_ponderado = (tabela_10['Beneficio / Custo'] * tabela_10['Total de Processos']).sum() / tabela_10['Total de Processos'].sum()

    # Criar um dicionário para a linha totalizadora
    linha_totalizadora = {
        'Grau de consenso': 'Total',  # Coluna renomeada anteriormente de "Modelo"
        'Escolha': 'Mista',
        '% da base de teste': tabela_10['% da base de teste'].sum(),
        'Composição Acumulada (%)': tabela_10['Composição Acumulada (%)'].max(),  # Usar o valor máximo de 'Composição Acumulada (%)
        'Beneficio / Custo': valor_ponderado,
        'Total de Processos': tabela_10['Total de Processos'].sum(),
        # Adicionar outras colunas que você queira totalizar ou preencher
    }

    # Converter o dicionário em DataFrame
    linha_totalizadora_df = pd.DataFrame([linha_totalizadora])

    # Adicionar a linha totalizadora ao DataFrame
    tabela_10 = pd.concat([tabela_10, linha_totalizadora_df], ignore_index=True)
    tabela_10['Beneficio / Custo']=tabela_10['Beneficio / Custo'].round(3)

    # Adiciona uma coluna 'Iteracao' para indicar o número da iteração
    tabela_10['Iteracao'] = rnd_ind

    # Concatenar a tabela_10 ao cons_tabela_10
    cons_tabela_10 = pd.concat([cons_tabela_10, tabela_10], ignore_index=True)


    # Cria uma cópia do DataFrame transformed_df
    tabela_11 = transformed_df.copy()

    # Filtra os registros para manter apenas os que contêm "-" na coluna "Modelo"
    tabela_11 = tabela_11[tabela_11['Modelo'].str.contains(r'11 -|10 -|9 -')]

    tabela_11['Modelo'] = tabela_11['Modelo'].str.split(' - ', expand=True)[1]

    # Cria coluna total de processos
    tabela_11['Total de Processos'] = tabela_11['support_0'] + tabela_11['support_1']
    tabela_11['Total de Processos']= tabela_11['Total de Processos'].astype(int)

    # Remove as colunas que contêm "f1-score", "precision", "recall", "Beneficio", "Custo"
    colunas_para_remover = [coluna for coluna in tabela_11.columns if any(substring in coluna for substring in ["f1-score", "precision", "recall", "Beneficio", "Custo", "Accuracy", "support"])]
    tabela_11 = tabela_11.drop(columns=colunas_para_remover)

    tabela_11 = tabela_11.groupby('Modelo', as_index=False).sum()

    # Cria a coluna "Beneficio / Custo" e arredonda para 2 casas decimais
    tabela_11['Beneficio / Custo'] = (tabela_11['TN'].astype(float) / tabela_11['FN'].astype(float)).round(3)

    tabela_11 = tabela_11.sort_values(by='Beneficio / Custo', ascending=False)

    # Adiciona uma coluna 'Iteracao' para indicar o número da iteração
    tabela_11['Iteracao'] = rnd_ind

    # Concatenar a tabela_10 ao cons_tabela_10
    cons_tabela_11 = pd.concat([cons_tabela_11, tabela_11], ignore_index=True)

    # Encontra o índice da linha com o maior valor na coluna 'Beneficio / Custo'
    indice_maior_valor = tabela_11['Beneficio / Custo'].idxmax()

    # Obtém o nome do modelo correspondente ao índice encontrado
    nome_modelo_maior_valor = tabela_11.loc[indice_maior_valor, 'Modelo']

    # Filtra as tabelas e adiciona uma coluna indicando a origem antes de combinar as linhas correspondentes
    tabela_12 = pd.concat([
        tabela_6[tabela_6['Modelo'] == nome_modelo_maior_valor].assign(Origem='Subconjunto Consenso 11'),
        tabela_7[tabela_7['Modelo'] == nome_modelo_maior_valor].assign(Origem='Subconjunto Consenso 10'),
        tabela_8[tabela_8['Modelo'] == nome_modelo_maior_valor].assign(Origem='Subconjunto Consenso 9'),
        tabela_9[tabela_9['Modelo'] == nome_modelo_maior_valor].assign(Origem='Subconjunto 8 ou menos')
    ], ignore_index=True)

    # Calcula a soma da coluna "Total de Processos"
    total_processos = tabela_12['Total de Processos'].sum()

    # Cria a coluna "% da base de teste" e arredonda para 1 casa decimal
    tabela_12['% da base de teste'] = (tabela_12['Total de Processos'].fillna(0) / total_processos * 100).round(2)

    tabela_12['teste'] = tabela_12.apply(
        lambda row: row['Beneficio / Custo'] if row['Origem'] != 'Subconjunto 8 ou menos' else 1.34,
        axis=1
    )
    tabela_12['Beneficio / Custo']=tabela_12['teste']
    tabela_12 = tabela_12.drop(columns=['Modelo','teste', 'TN', 'FP', 'FN', 'TP'])

    # Cria a coluna "Composição Acumulada (%)"
    tabela_12['Composição Acumulada (%)'] = tabela_12['% da base de teste'].cumsum()

    # Cria a coluna "Escolha" com base na condição
    tabela_12['Escolha'] = tabela_12.apply(
        lambda row: nome_modelo_maior_valor if row['Origem'] != 'Subconjunto 8 ou menos' else 'Aleatória',
        axis=1
    )

    # Renomeia a coluna "Modelo" para "Grau de consenso"
    tabela_12 = tabela_12.rename(columns={'Origem': 'Grau de consenso'})

    # Reordena as colunas
    tabela_12 = tabela_12[['Grau de consenso', 'Escolha', 'Total de Processos', '% da base de teste', 'Composição Acumulada (%)', 'Beneficio / Custo']]

    # Calcular o valor ponderado da coluna "Beneficio / Custo" em função da coluna "Total de Processos"
    valor_ponderado = (tabela_12['Beneficio / Custo'] * tabela_12['Total de Processos']).sum() / tabela_12['Total de Processos'].sum()

    # Criar um dicionário para a linha totalizadora
    linha_totalizadora = {
        'Grau de consenso': 'Total',  # Coluna renomeada anteriormente de "Modelo"
        'Escolha': 'Mista',
        '% da base de teste': tabela_12['% da base de teste'].sum(),
        'Composição Acumulada (%)': tabela_12['Composição Acumulada (%)'].max(),  # Usar o valor máximo de 'Composição Acumulada (%)
        'Beneficio / Custo': valor_ponderado,
        'Total de Processos': tabela_12['Total de Processos'].sum(),
        # Adicionar outras colunas que você queira totalizar ou preencher
    }

    # Converter o dicionário em DataFrame
    linha_totalizadora_df = pd.DataFrame([linha_totalizadora])

    # Adicionar a linha totalizadora ao DataFrame
    tabela_12 = pd.concat([tabela_12, linha_totalizadora_df], ignore_index=True)
    tabela_12['Beneficio / Custo']=tabela_12['Beneficio / Custo'].round(3)

    # Adiciona uma coluna 'Iteracao' para indicar o número da iteração
    tabela_12['Iteracao'] = rnd_ind

    # Concatenar a tabela_12 ao cons_tabela_12
    cons_tabela_12 = pd.concat([cons_tabela_12, tabela_12], ignore_index=True)

# Especifica o caminho e o nome do arquivo xlsx
caminho_arquivo = '/content/drive/MyDrive/MESP_Previdencia/'
cons_transformed_df.to_excel(caminho_arquivo + "cons_transformed_df_2.xlsx", index=False)
cons_tabela_5.to_excel(caminho_arquivo + "cons_tabela_5_2.xlsx", index=False)
cons_tabela_10.to_excel(caminho_arquivo + "cons_tabela_10_2.xlsx", index=False)
cons_tabela_11.to_excel(caminho_arquivo + "cons_tabela_11_2.xlsx", index=False)
cons_tabela_12.to_excel(caminho_arquivo + "cons_tabela_12_2.xlsx", index=False)

# Especifica o caminho e o nome do arquivo xlsx
caminho_arquivo = '/content/drive/MyDrive/MESP_Previdencia/'
cons_transformed_df.to_excel(caminho_arquivo + "cons_transformed_df.xlsx", index=False)
cons_tabela_5.to_excel(caminho_arquivo + "cons_tabela_5.xlsx", index=False)
cons_tabela_10.to_excel(caminho_arquivo + "cons_tabela_10.xlsx", index=False)
cons_tabela_11.to_excel(caminho_arquivo + "cons_tabela_11.xlsx", index=False)
cons_tabela_12.to_excel(caminho_arquivo + "cons_tabela_12.xlsx", index=False)

import pandas as pd

# Carregar o arquivo do Excel
arquivo_excel = pd.ExcelFile('/content/drive/MyDrive/MESP_Previdencia/cons_transformed_df.xlsx')

cons_transformed_df = arquivo_excel.parse()

arquivo_excel_2 = pd.ExcelFile('/content/drive/MyDrive/MESP_Previdencia/cons_transformed_df_2.xlsx')


# Carregar a nova planilha Excel em um novo dataframe
df_novo = pd.read_excel(arquivo_excel_2)

# Combinar os dataframes (acrescentar as linhas do novo dataframe ao existente)
cons_transformed_df = pd.concat([cons_transformed_df, df_novo], ignore_index=True)

# Exibir as primeiras linhas do dataframe combinado
cons_transformed_df.shape

colunas_para_remover = [coluna for coluna in cons_transformed_df.columns if any(substring in coluna for substring in ["f1-score", "precision", "recall", "Accuracy", "support"])]

cons_transformed_df = cons_transformed_df.drop(columns=colunas_para_remover)

colunas_para_remover = ['Beneficio', 'Custo']

cons_transformed_df = cons_transformed_df.drop(columns=colunas_para_remover)

import pandas as pd

cons_transformed_df_coletivas = cons_transformed_df[cons_transformed_df['Modelo'].str.contains('Consenso') & ~cons_transformed_df['Modelo'].str.contains('-')]

# Filtrar as linhas onde "Modelo" contém '6', '7' ou '8'
filtered_678df = cons_transformed_df_coletivas[cons_transformed_df_coletivas['Modelo'].str.contains('6|7|8')]

grouped_678df = filtered_678df.groupby(['Iteracao']).agg({
    'TN': 'sum',
    'FP': 'sum',
    'FN': 'sum',
    'TP': 'sum'
}).reset_index()

# Adicionar uma coluna "Modelo" indicando o agrupamento
grouped_678df['Modelo'] = 'Subconjunto Consenso 8 ou menos modelos'
grouped_678df['Beneficio/Custo'] = grouped_678df['TN'] / grouped_678df['FN']

# Filtrar as linhas onde "Modelo" contém '9', '10' ou '11'
filtered_91011df = cons_transformed_df_coletivas[cons_transformed_df_coletivas['Modelo'].str.contains('9|10|11')]

# Agrupar por "Iteracao" e somar as colunas desejadas, mantendo o "Modelo"
grouped_91011df = filtered_91011df.groupby(['Iteracao']).agg({
    'TN': 'sum',
    'FP': 'sum',
    'FN': 'sum',
    'TP': 'sum'
}).reset_index()

# Adicionar uma coluna "Modelo" indicando o agrupamento
grouped_91011df['Modelo'] = 'Subconjunto Consenso 9 ou mais modelos'
grouped_91011df['Beneficio/Custo'] = grouped_91011df['TN'] / grouped_91011df['FN']

# Adicionar as linhas derivadas do agrupamento ao dataframe original
cons_transformed_df_coletivas = pd.concat([cons_transformed_df_coletivas, grouped_678df, grouped_91011df], ignore_index=True)

import pandas as pd
import scipy.stats as stats
import numpy as np

# Supondo que o dataframe se chame cons_transformed_df
# Função para calcular a margem de erro e o intervalo de confiança
def calc_confidence_interval(data, confidence=0.95):
    n = len(data)
    mean = np.mean(data)
    stderr = stats.sem(data)  # Calcula o erro padrão da média
    margin_of_error = stderr * stats.t.ppf((1 + confidence) / 2., n-1)
    margin_of_error_percent = (margin_of_error / mean) * 100 if mean != 0 else np.inf
    return f"{mean:.2f} ± {margin_of_error:.2f} ({margin_of_error_percent:.2f}%)"

# Aplicar a função a cada coluna (exceto "Modelo") para calcular a média e o intervalo de confiança
confidence_intervals = cons_transformed_df_coletivas.groupby('Modelo').agg(lambda x: calc_confidence_interval(x)).reset_index()

colunas_para_remover = ['Iteracao']

ic_escolhacoletiva = confidence_intervals.drop(columns=colunas_para_remover)

ic_escolhacoletiva.head(20)

# Filtrar as linhas onde "Modelo" contém '6', '7' ou '8'
cons_transformed_df_ind = cons_transformed_df[cons_transformed_df['Modelo'].str.contains(' - ')]

filtered_678df = cons_transformed_df_ind[cons_transformed_df_ind['Modelo'].str.contains('6|7|8')]

filtered_678df['Nome_Modelo'] = filtered_678df['Modelo'].str.split(' - ').str[-1]

grouped_678df = filtered_678df.groupby(['Iteracao', 'Nome_Modelo']).agg({
    'TN': 'sum',
    'FP': 'sum',
    'FN': 'sum',
    'TP': 'sum'
}).reset_index()

# Adicionar uma coluna "Modelo" indicando o agrupamento
grouped_678df["Modelo"] = "Subconjunto Consenso 8 ou menos" + " - " + grouped_678df["Nome_Modelo"]
grouped_678df['Beneficio/Custo'] = grouped_678df['TN'] / grouped_678df['FN']

# Adicionar as linhas derivadas do agrupamento ao dataframe original
cons_transformed_df_ind = pd.concat([cons_transformed_df_ind, grouped_678df], ignore_index=True)

import pandas as pd
import scipy.stats as stats
import numpy as np

cons_transformed_df_ind = cons_transformed_df_ind.drop(columns=["Nome_Modelo"])

# Supondo que o dataframe se chame cons_transformed_df
# Função para calcular a margem de erro e o intervalo de confiança
def calc_confidence_interval(data, confidence=0.95):
    n = len(data)
    mean = np.mean(data)
    stderr = stats.sem(data)  # Calcula o erro padrão da média
    margin_of_error = stderr * stats.t.ppf((1 + confidence) / 2., n-1)
    margin_of_error_percent = (margin_of_error / mean) * 100 if mean != 0 else np.inf
    return f"{mean:.2f} ± {margin_of_error:.2f} ({margin_of_error_percent:.2f}%)"

# Aplicar a função a cada coluna (exceto "Modelo") para calcular a média e o intervalo de confiança
confidence_intervals_ind = cons_transformed_df_ind.groupby('Modelo').agg(lambda x: calc_confidence_interval(x)).reset_index()

colunas_para_remover = ['Iteracao']

ic_escolhaindividual = confidence_intervals_ind.drop(columns=colunas_para_remover)
ic_escolhaindividual.head(20)

import os
import sqlite3

# Caminho para a pasta no Google Drive
db_path = '/content/drive/MyDrive/MESP_Previdencia/mesp_previdencia.db'


conexao = sqlite3.connect(db_path)

# Use o método to_sql para escrever o DataFrame no banco de dados
# Substitua 'tabela_final' pelo nome que deseja para a tabela
ic_escolhacoletiva.to_sql('ic_escolhacoletiva', conexao, index=True, if_exists='replace')

import os
import sqlite3

# Caminho para a pasta no Google Drive
db_path = '/content/drive/MyDrive/MESP_Previdencia/mesp_previdencia.db'


conexao = sqlite3.connect(db_path)

# Use o método to_sql para escrever o DataFrame no banco de dados
# Substitua 'tabela_final' pelo nome que deseja para a tabela
ic_escolhaindividual.to_sql('ic_escolhaindividual', conexao, index=True, if_exists='replace')

import pandas as pd

# Especifica o caminho e o nome do arquivo xlsx
caminho_arquivo = '/content/drive/MyDrive/MESP_Previdencia/ic_escolhacoletiva3.xlsx'

# Exporta o DataFrame para o arquivo xlsx
ic_escolhacoletiva.to_excel(caminho_arquivo, index=False)

print(f'DataFrame exportado com sucesso para {caminho_arquivo}')

import pandas as pd

# Especifica o caminho e o nome do arquivo xlsx
caminho_arquivo = '/content/drive/MyDrive/MESP_Previdencia/ic_escolhaindividual3.xlsx'

# Exporta o DataFrame para o arquivo xlsx
ic_escolhaindividual.to_excel(caminho_arquivo, index=False)

print(f'DataFrame exportado com sucesso para {caminho_arquivo}')